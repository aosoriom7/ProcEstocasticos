{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c086e636-c2a9-417a-9a82-91d265b81188",
   "metadata": {},
   "source": [
    "### Training an [unsupervised] neural network to implement the discrete Fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b270a5-cf99-464a-abbc-5183d8cda85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://publik.tuwien.ac.at/files/PubDat_171587.pdf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data:\n",
    "# N signallength\n",
    "\n",
    "N = 512\n",
    "batch = 8192\n",
    "\n",
    "t = np.linspace(0, 10, N, endpoint=True)\n",
    "xsin = np.sin(t/4)\n",
    "\n",
    "# Initialise weight vector to train\n",
    "# Generate random input data and desired output data\n",
    "sig = np.random.randn(batch, N) + 1j*np.random.randn(batch, N) + np.pi*np.abs(xsin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cff662-e87d-4942-8445-88f73b7d5c33",
   "metadata": {},
   "source": [
    "$\\begin{align*}y_k &= \\displaystyle \\sum_{n=0}^{N-1} x_n \\cdot \\exp(-i\\frac{2 \\pi k}{N}n)\\\\\n",
    "\\boldsymbol{y} &= \\boldsymbol{x} W  \\tag{2}\n",
    "\\end{align*}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a196178-9768-4f94-a998-76ce4977ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('Data/DFT.gif', width=500)\n",
    "\n",
    "# Expected weights to match\n",
    "# Compute Fourier transform using the conventional Fast FT method \n",
    "F = np.fft.fft(sig, axis=-1)\n",
    "\n",
    "# First half of inputs/outputs is real part, second half is imaginary part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293467b6-0a60-4dc9-903f-feef09536894",
   "metadata": {},
   "source": [
    "$\\exp(-i \\frac{2 \\pi k}{N}n) = cos(\\frac{2 \\pi k}{N}n) - i sin(\\frac{2 \\pi k}{N}n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbff41-2e5b-4b98-9731-916fc651348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([sig.real, sig.imag])\n",
    "Y = np.hstack([F.real, F.imag])\n",
    "\n",
    "# Create model with no hidden layers, same number of outputs as inputs.\n",
    "# Zero-valued bias needed.  No activation function, since DFT is linear.\n",
    "model = Sequential([Dense(N*2, input_dim=N*2, use_bias=False)])\n",
    "# A Sequential model is appropriate for a plain stack of layers \n",
    "# where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "# Compile model\n",
    "# it configures the model for training.\n",
    "model.compile(loss='mean_squared_error', optimizer='adamax')\n",
    "# loss='mean_squared_error'\n",
    "\n",
    "# Fit the model\n",
    "# It rains the model for a fixed number of epochs (iterations on a dataset).\n",
    "history =model.fit(X, Y, epochs=128, batch_size=128, verbose=0)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.title('Model Loss')\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Mean Squared Error'])\n",
    "plt.show()\n",
    "\n",
    "# Confirm that it works\n",
    "data = np.arange(N) # harmonics [0 N-1]\n",
    "\n",
    "def ANN_DFT(x):\n",
    "    if len(x) != N:\n",
    "        raise ValueError(f'Input must be length {N}')\n",
    "    pred = model.predict(np.hstack([x.real, x.imag])[np.newaxis])[0]\n",
    "    result = pred[:N] + 1j*pred[N:]\n",
    "    return result\n",
    "\n",
    "ANN = ANN_DFT(data)\n",
    "#print(np.abs(ANN))\n",
    "\n",
    "FFT = np.fft.fft(data)\n",
    "print(f'ANN matches FFT: {np.allclose(ANN, FFT)}')\n",
    "\n",
    "# Heat map of neuron weights\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "plt.imshow(model.get_weights()[0], vmin=-1, vmax=1, cmap='coolwarm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc50d3-793b-4d76-80c9-fc62daa93cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed signal fft, learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb66e742-1b5d-41a2-a4b6-2d2e78e22883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something as basic as a square or a square root operation requires hidden layer networks and a disproportionately large number of nodes to solve accurately\n",
    "# As a result,  neural networks struggle with even a simpler transforms  like FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb153cd-def8-40d3-858a-20ee91631a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task : bayes estimator "
   ]
  },
  {
   "cell_type": "raw",
   "id": "26727866-979f-4f76-936f-15b7af229e86",
   "metadata": {},
   "source": [
    "Linear Regression \n",
    "    Training an [unsupervised] neural network to implement the discrete Fourier transform\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
